

# Tutorial_0723_after

### 단어 벡터화 ⚖️

```
띄어쓰기나 맞춤법에 따라 다른 의미 -> 그래서 텍스트 전처리가 필요!
```



---

### [ 텍스트 데이터 정제 및 전처리] 🏸

#### (1) 데이터 정제 및 전처리  

- 기계가 텍스트를 이해할 수 있도록 텍스트를 정제

- 신호와 소음을 구분

- 아웃라이어데이터로 인한 오버피팅을 방지

  * HTML Tag, 특수문자, 이모티콘

  * 정규표현식

  * 불용어 (stopword)

    ```
    ex) 조사, 접미사 -나, 너, 은, 는, 이, 가, 하다, 합니다 등
    ```

    * 일반적으로 코퍼스에서 자주 나타나는 단어로, 학습이나 예측 프로세스에 실제로 기여하지 않는 단어들.
    * 영어는 불용어 Dataset 잘 되어 있지만 한국어에 쓸만한 건 없다.

  * 어간추출 (stemming)

    ```
    ex) 새로운, 새로울 -> 새롭다
        먹었다, 먹을, 먹을지도 모르는 -> 먹다
    ```

   * 단어를 축약형으로 바꿔준다. 의미가 통하도록 축약하여 어간을 추출한다.

  * 음소표기법 (lemmatizing)

    * 어간추출을 했을 때 의미가 부딪힐 위험이 있는 동음이의어 등의 단어들을 처리하는 방법
    * 품사 정보가 보존된 형태의 기본형으로 변환

    

> 정규화(normalization)
> 토큰화(tokenization)
> 어근화(stemming)
> 어구추출(phrase extraction)



----


### [ 텍스트 데이터 벡터화 ]  🍡

> 머신러닝 알고리즘은, 0과 1밖에 모르는 기계에게 인간의 언어를 알려주기 위한 절차이다. 컴퓨터는 숫자만 인식할 수 있기 때문에 수치로 바꾸어 바이너리 코드로 처리해주어야 한다.



#### (1) 텍스트 데이터, 범주형 데이터 => 수치형데이터

```
- 머신러닝이나 딥러닝 알고리즘은 수치로된 데이터만 이해
```



#### (2) One Hot Vector

* 텍스트 데이터나 범주형 데이터를 수치형 데이터로 바꾸어 주어야 한다. 벡터에서 해당되는 하나의 데이터만 1로 변경해 주고, 나머지는 0으로 채워준다. 이 행렬을 Random Forests와 Decision Tree에 넣어준다.



#### (3) BOW (bag of words)

* 가장 간단하지만 효과적이라 널리쓰인다.
* 구조에 상관없이 단어의 출현횟수만 센다.

* 단어는 같지만 문장 구조나 순서가 달라 뜻이 완전히 달라지는 문장들을 완전히 동일하게 반환한다는 단점이 있다. 이를 보완하기 위해 n-gram을 사용하여 의미가 보존되도록 n개의 토큰을 사용한다.
* n-gram : uni-gram, bi-gram, tri-gram 등으로 구분. (묶어주는 토큰의 개수에 따라). bi-gram (1,2)나 tri-gram(2,3) 등을 사용하면 토큰 개수를 섞어서 사용할 수 있다.



#### (4) TF-IDF (Term frequency Inverse document frequency)

> #### TF (단어 빈도, term frequency)
>
> : 특정한 단어가 문서 내에서 얼마나 자주 등장하는지를 나타내는 값 
>
> 이 값이 높을 수록 문서에서 중요한 단어라고 생각할 수 있지만 단어 자체가 문서군 내에서 자주 사용되는 경우 이것은 그 단어가 흔하게 등장한다는 것을 의미



> #### DF (문서 빈도, document frequency)
>
> : 문서군 내에서 자주 사용됨
>
> #### IDF (역문서 빈도, inverse document frequency)
>
> : 값의 역수

* 이를테면 전체 청원에서 '초등학교'라는 키워드는 자주 등장하지 않지만, 보육과 관련된 특정 청원에서는 이 단어가 자주 등장한다.
* TF-IDF는 TF와 IDF를 곱한 값이다.



#### (4) Word2Vec

* 딥러닝 기반 알고리즘.
* 유사한 벡터를 근처에 배치하기 때문에, 추천 시스템에도 많이 쓰인다.
* CBOW (continuous bag-of-words), Skip-Gram
  * CBOW : 전체 텍스트로 하나의 단어를 예측하기 때문에, 작은 데이터셋일 수록 유리하다.
  * Skip-Gram : 타겟 단어들로부터 원본 단어를 역으로 예측하는 것으로, CBOW와는 반대로 컨텍스트-타겟 쌍을 새로운 발견으로 처리하고 큰 규모의 데이터셋일수록 유리하다.

>  vector size가 매우 크로 sparse하므로 neural net 성능이 잘 나오지 X



---


### [ 텍스트 데이터 시각화] 💎

> - 워드클라우드
> - 청와대 국민청원 데이터
> - IMDB 영화뷰 데이터
> - 단어 수 혹은 문장길이, 특수문자, 불용어 갯수 등을 시각화
> - 작가별 품사 사용에 대한 시각화
> - 작가별 단어개수 시각화



 ##### (1) Word2Vec으로 벡터화하고, 일부 데이터를 차원축소 기법으로 줄여서 표현

  *  비슷한 단어끼리 비슷한 위치에 분포
  *  조사와 불용어가 섞여 있어 데이터 정제 필요

 **(2) 작가별 품사 사용, 단어 개수, 소설에 자주 등장하는 단어**



----



### [ 자연어 처리로 할 수 있는 일 ] 👉

- 자동 요약, 맞춤법 수정, 스팸메일 검출, 분류, 자동답변, 고객센터, 챗봇, 기계번역, 추천, 감정분석 등
- 자연어 처리에서 활용하기
  - Classification : 스팸메일 분류
  - Regression : 리뷰 평점 예측
  - Clustering : 비슷한 메일끼리 모으기
  - Dimensionality reduction : 차원 축소 기법으로 시각화
